---
title: "Autocorrelation_stats"
author: "Debora"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
```

## Load and Preprocess Datasets

We load two datasets: one with a preparation technique (repeated frond selection) and one without.

```{r}
# This dataset contains replicates for which a preparation technique was performed (repeated first born frond selection to reduce maternal effects)
original_dataset_2 <- read.csv("https://raw.githubusercontent.com/Cuddington-Lab/thermal-experiments/main/dataset_autocorrelation_2022-2023.csv",header=TRUE, stringsAsFactors = TRUE,fileEncoding="UTF-8-BOM")
original_dataset_2$prep <- rep("yes",times=length(original_dataset_2$Experiment_Number))

# This dataset contains replicates of experiments performed without a preparation technique 
original_dataset_1 <- read.csv("https://raw.githubusercontent.com/Cuddington-Lab/thermal-experiments/main/dataset_autocorrelation_2021-2022.csv",header=TRUE, stringsAsFactors = TRUE,fileEncoding="UTF-8-BOM")
original_dataset_1$prep <- rep("no",times=length(original_dataset_1$Experiment_Number))
```

## Combine Datasets

We combine both datasets and filter out rows based on specific conditions for standard deviation (`Obs_sd`) and autocorrelation (`Obs_ac`) to clean the data.

```{r}
# Blending both datasets, as there are no significant differences between preparation methods (probably because we only have 2 methods in low temperatures)
datin <- rbind(original_dataset_1,original_dataset_2)

datin <- datin[!(datin$Treatment == 0 & (datin$Obs_sd <= 2.1 | datin$Obs_sd >= 2.9))
               &!(datin$Treatment == 0.95 & (datin$Obs_sd <= 2.1 | datin$Obs_sd >= 2.9)),]

datin <- datin[!(datin$Treatment == 0 & (datin$Obs_ac <= -0.2 | datin$Obs_ac >= 0.2))
               &!(datin$Treatment == 0.95 & (datin$Obs_ac <= 0.92 | datin$Obs_ac >= 0.98)),]

datin <- subset(datin, !Errors == "y"|is.na(Errors))
datin <- subset(datin, !Treatment == "constant")
```


```{r include=FALSE}
## Data Relabeling
# We relabel the groups for easier visualization and manipulate the dataset to  prepare it for analysis.

colnames(datin)[6] <- "Label"
table(datin$Label, datin$cat_1)
levels(datin$cat_1) = c("no autocorrelation","hot-cold","no autocorrelation","cold-hot")
datin$Treatment<-paste0(datin$cat_1)

# Relabel groups to further simplify visualization of results
library(stringr)
datin$Label <- str_replace(datin$Label, "0.95", "strong autocorrelation")
datin$Label <- str_replace(datin$Label, "0", "no autocorrelation")

# Extract experiment run IDs
datin$Exp_run <- paste0(datin$Mean_temperature, str_sub(datin$Profile_name, -2, -1))
datin$Exp_run <- gsub('_', '', datin$Exp_run)
datin$Exp_run <- as.numeric(datin$Exp_run)
```


```{r include=FALSE}
## Subset Data
# We subset the data to retain only those experiment runs that have at least 3 replicates within each `Mean_Temperature` and `Species` level

datin <- datin[
  ave(seq_along(datin$Exp_run), 
      interaction(datin$Exp_run, datin$Mean_temperature, datin$Species), 
      FUN = length) >= 3, 
]
```

```{r include=FALSE}
## Further Data Processing

# Blending 10C and 15C, as there are no significant differences between performance in these 2 average temperatures
datin$Mean_temperature <- str_replace(datin$Mean_temperature, "15", "10")
datin$Mean_temperature <- str_replace(datin$Mean_temperature, "10", "10-15")

# Removing "lab" strain, as preliminary statistical analyses showed significant differences between the 2 common duckweed strains used in this experiment in average temperature of 27C
datin <- subset(datin, !(Species == "Lab_LM" & Mean_temperature == "27"))

# Blending common duckweed strains, as preliminary analyses showed no differences between them
datin$Species <- gsub("Lab_LM|Field_LM", "LM", datin$Species)
```


## View dataset and response variable
```{r echo=FALSE}
datin$change_frond_count <- (datin$Frond_count_1 + datin$Frond_count_2 + datin$Frond_count_3)-12
datin$total_living_fronds <- (datin$Frond_count_1 + datin$Frond_count_2 + datin$Frond_count_3)
knitr::kable(tail(datin[, c(8, 34, 33, 36)]))

dataset <- datin
dataset <- within(dataset, Mean_temperature <- relevel(factor(Mean_temperature), ref = "10-15"))
dataset <- within(dataset, Treatment <- relevel(factor(Treatment), ref = "no autocorrelation"))
dataset <- within(dataset, Species <- relevel(factor(Species), ref = "LM"))

dataset$Experiment_Number <- as.factor(dataset$Experiment_Number)
dataset$Incubator <- as.factor(dataset$Incubator)
dataset$Mean_temperature <- as.factor(dataset$Mean_temperature)
dataset$Treatment <- as.factor(dataset$Treatment)
dataset$Label <- as.factor(dataset$Label)
```

Poisson selected because this is count data, right-skewed (lots of values close to zero)
```{r echo=FALSE}
dataset$total_living_fronds <- as.numeric(dataset$total_living_fronds)
par(mfrow = c(2, 2))
invisible(sapply(unique(dataset$Label), function(trt) {
  invisible(sapply(c("LM", "LP"), function(sp) {
    hist(dataset$total_living_fronds[dataset$Species == sp & dataset$Label == trt],
         main = paste("Species:", sp, "\nTreatment:", trt),  
         xlab = "Live fronds", col = "lightblue", border = "white",
         breaks = seq(0, max(dataset$total_living_fronds, na.rm = TRUE) + 10, by = 10))
  }))
}))
```

## View number of replicates
LM
```{r echo=FALSE}
table(dataset$Mean_temperature[dataset$Species == "LM"], dataset$Treatment[dataset$Species == "LM"])
```

LP
```{r echo=FALSE}
table(dataset$Mean_temperature[dataset$Species == "LP"], dataset$Treatment[dataset$Species == "LP"])
```

### Model fitting and validation

```{r echo=TRUE}
model_results <- list()
compare_results <- list()

species_list <- list("LM", "LP")

for (sp in species_list) {
  species_data <- subset(dataset, Species == sp)
  
  # Assigning new IDs to experiment runs to ensure IDs are unique (as different
  # average temperatures had same IDs)
  species_data$Exp_run <- rep(seq(1, length(species_data$Exp_run) / 3), each = 3)

  library(lme4)

# Fit the models
simple <- glm(total_living_fronds ~ Label*Mean_temperature, data=species_data, family=poisson)
exp_number <- glmer(total_living_fronds ~ Label*Mean_temperature + (1|Exp_run), data=species_data, family=poisson, control = glmerControl(optimizer = "nloptwrap"))

# Compare simple and mixed model
anova(exp_number, simple, test = "Chisq")
L0 <- logLik(simple)
L1 <- logLik(exp_number)
L.ratio <- as.vector(- 2 * (L0 - L1))
p_value <- 0.5 * (1 - pchisq(L.ratio, 1))
options(scipen = 999)
print(paste("Comparing simple x mixed model:"))
cat("p_value:", format(p_value, digits = 10), "\n")

# Store models
model_results[[sp]] <- list(simple = simple, exp_number = exp_number)

# Compare models by AIC
Cand.modsF <- list("no random effects" = model_results[[sp]]$simple,
                   "experiment number" = model_results[[sp]]$exp_number)

# Get AIC for each model and store it
AIC_values <- sapply(Cand.modsF, function(model) AIC(model))

# Create a summary table
compare_results[[sp]] <- knitr::kable(data.frame(Model = names(AIC_values), AIC = AIC_values), "simple", digits = 2)

 cat("Model assessment:", sp, "\n")
 print(compare_results[[sp]])

# Find the best model based on AIC
best_model_name <- names(AIC_values)[which.min(AIC_values)]
model_results[[sp]] <- Cand.modsF[[best_model_name]]

  best_model <- model_results[[sp]]
  
  model_dispersion <- sum(residuals(best_model, type = "pearson")^2) / df.residual(best_model)
  print(paste("Model dispersion ratio for species", sp, ":", model_dispersion))
  
  cat("Wald test type 3 for significance of predictor:", sp, "\n")
  library(car)
phi <- sum(residuals(best_model, type="pearson")^2) / df.residual(best_model)  
anova_table <- Anova(best_model, type = "III")  

# Adjusting chi-square values (quasipoisson)
anova_table$`Chisq` <- anova_table$`Chisq` / phi  

# Recalculating p-values using chi-square distribution
anova_table$`Pr(>Chisq)` <- pchisq(anova_table$`Chisq`, anova_table$Df, lower.tail = FALSE)  

print(anova_table)
}
```

## Model validation

All assumptions were met.

```{r echo=FALSE}
validation_results <- list()

for (sp in species_list) {
  best_model <- model_results[[sp]]
  # Model validation for each species
  model_dispersion <- sum(residuals(best_model, type = "pearson")^2) / df.residual(best_model)
  print(paste("Model dispersion ratio for species", sp, ":", model_dispersion))
  
  # Deviance of the best model
best_model_deviance <- deviance(best_model)

species_data <- subset(dataset, Species == sp)
# Fit the null model (only the intercept)
null_model <- glm(total_living_fronds ~ 1, family = poisson, data = species_data)

# Deviance of the null model
null_model_deviance <- deviance(null_model)

# Calculate pseudo R^2 based on deviance
pseudo_R2_deviance <- 1 - (best_model_deviance / null_model_deviance)

# Print the results
cat(sp, "\n")
cat("Deviance of the best model:", best_model_deviance, "\n")
cat("Deviance of the null model:", null_model_deviance, "\n")
cat("Pseudo R^2 based on deviance:", pseudo_R2_deviance, "\n")

# Residual Diagnostics
par(mfrow = c(2, 2))
plot(fitted(best_model), residuals(best_model), 
     xlab = "Fitted Values", ylab = "Residuals", main = "Residuals vs Fitted")
abline(h = 0, col = "red")
hist(residuals(best_model, type = "deviance"), 
     main = "Deviance Residuals", xlab = "Residuals")
cooks_dist <- cooks.distance(best_model)
plot(cooks_dist, type = "h", main = "Cook's Distance", ylab = "Cook's Distance")
abline(h = 4 / nrow(species_data), col = "red", lty = 2)
}
```

# Adjusting model results to account for overdispersion
```{r echo=TRUE}
for (sp in species_list) {
  full_mod1 <- model_results[[sp]]
quasi_table <- function(model,ctab=coef(summary(model))) {
  phi <- sum(residuals(model, type="pearson")^2)/df.residual(model)
  qctab <- within(as.data.frame(ctab),
                  {   `Std. Error` <- `Std. Error`*sqrt(phi)
                  `z value` <- Estimate/`Std. Error`
                  `Pr(>|z|)` <- 2*pnorm(abs(`z value`), lower.tail=FALSE)
                  })
  return(qctab)
}

print(paste("Adjusted model summary - species :", sp, ":"))
printCoefmat(quasi_table(full_mod1),digits=2)
#As specified, the estimates are identical; 
#the standard errors and p-values have been appropriately 
#inflated, the z-values have been appropriately deflated.
#https://stackoverflow.com/questions/68915173/how-do-i-fit-a-quasi-poisson-model-with-lme4-or-glmmtmb
# Checks for Overdispersion: It calculates a number (phi) that tells you how much the data spreads out more than expected.
# Fixes Standard Errors: It takes the standard errors from the model and makes them bigger based on phi.
# Recalculates Other Numbers: Since standard errors changed, it also updates the z-values and p-values.
# Returns the New Table: It gives you a corrected version of the table with the adjusted values.
# Basically, your Poisson model didn’t fit well because the data was too spread out, so this function helps fix that by making sure the errors and test results reflect the extra spread
}
```
