---
title: "Autocorrelation_stats"
author: "Debora"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
  pdf_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	echo = TRUE,
	message = FALSE,
	warning = FALSE
)
```

## Load and Preprocess Datasets

We load two datasets: one with a preparation technique (repeated frond selection) and one without.

```{r}
# This dataset contains replicates for which a preparation technique was performed (repeated first born frond selection to reduce maternal effects)
original_dataset_2 <- read.csv("https://raw.githubusercontent.com/Cuddington-Lab/thermal-experiments/main/dataset_autocorrelation_2022-2023.csv",header=TRUE, stringsAsFactors = TRUE,fileEncoding="UTF-8-BOM")
original_dataset_2$prep <- rep("yes",times=length(original_dataset_2$Experiment_Number))

# This dataset contains replicates of experiments performed without a preparation technique 
original_dataset_1 <- read.csv("https://raw.githubusercontent.com/Cuddington-Lab/thermal-experiments/main/dataset_autocorrelation_2021-2022.csv",header=TRUE, stringsAsFactors = TRUE,fileEncoding="UTF-8-BOM")
original_dataset_1$prep <- rep("no",times=length(original_dataset_1$Experiment_Number))
```

## Combine Datasets

We combine both datasets and filter out rows based on specific conditions for standard deviation (`Obs_sd`) and autocorrelation (`Obs_ac`) to clean the data.

```{r}
# Blending both datasets, as there are no significant differences between preparation methods (probably because we only have 2 methods in low temperatures)
datin <- rbind(original_dataset_1,original_dataset_2)

datin <- datin[!(datin$Treatment == 0 & (datin$Obs_sd <= 2.1 | datin$Obs_sd >= 2.9))
               &!(datin$Treatment == 0.95 & (datin$Obs_sd <= 2.1 | datin$Obs_sd >= 2.9)),]

datin <- datin[!(datin$Treatment == 0 & (datin$Obs_ac <= -0.2 | datin$Obs_ac >= 0.2))
               &!(datin$Treatment == 0.95 & (datin$Obs_ac <= 0.92 | datin$Obs_ac >= 0.98)),]

datin <- subset(datin, !Errors == "y"|is.na(Errors))
datin <- subset(datin, !Treatment == "constant")
```


```{r include=FALSE}
## Data Relabeling
# We relabel the groups for easier visualization and manipulate the dataset to  prepare it for analysis.

colnames(datin)[6] <- "Label"
table(datin$Label, datin$cat_1)
levels(datin$cat_1) = c("no autocorrelation","hot-cold","no autocorrelation","cold-hot")
datin$Treatment<-paste0(datin$cat_1)

# Relabel groups to further simplify visualization of results
library(stringr)
datin$Label <- str_replace(datin$Label, "0.95", "strong autocorrelation")
datin$Label <- str_replace(datin$Label, "0", "no autocorrelation")

# Extract experiment run IDs
datin$Exp_run <- paste0(datin$Mean_temperature, str_sub(datin$Profile_name, -2, -1))
datin$Exp_run <- gsub('_', '', datin$Exp_run)
datin$Exp_run <- as.numeric(datin$Exp_run)
```


```{r include=FALSE}
## Subset Data
# We subset the data to retain only those experiment runs that have at least 3 replicates within each `Mean_Temperature` and `Species` level

datin <- datin[
  ave(seq_along(datin$Exp_run), 
      interaction(datin$Exp_run, datin$Mean_temperature, datin$Species), 
      FUN = length) >= 3, 
]
```

```{r include=FALSE}
## Further Data Processing

# Blending 10C and 15C, as there are no significant differences between performance in these 2 average temperatures
datin$Mean_temperature <- str_replace(datin$Mean_temperature, "15", "10")
datin$Mean_temperature <- str_replace(datin$Mean_temperature, "10", "10-15")

# Removing "lab" strain, as preliminary statistical analyses showed significant differences between the 2 common duckweed strains used in this experiment in average temperature of 27C
datin <- subset(datin, !(Species == "Lab_LM" & Mean_temperature == "27"))

# Blending common duckweed strains, as preliminary analyses showed no differences between them
datin$Species <- gsub("Lab_LM|Field_LM", "LM", datin$Species)
```


## View dataset and response variable
```{r echo=FALSE}
datin$change_frond_count <- (datin$Frond_count_1 + datin$Frond_count_2 + datin$Frond_count_3)-12
datin$total_living_fronds <- (datin$Frond_count_1 + datin$Frond_count_2 + datin$Frond_count_3)
knitr::kable(tail(datin[, c(8, 34, 33, 36)]))

dataset <- datin
dataset <- within(dataset, Mean_temperature <- relevel(factor(Mean_temperature), ref = "10-15"))
dataset <- within(dataset, Treatment <- relevel(factor(Treatment), ref = "no autocorrelation"))
dataset <- within(dataset, Species <- relevel(factor(Species), ref = "LM"))

dataset$Experiment_Number <- as.factor(dataset$Experiment_Number)
dataset$Incubator <- as.factor(dataset$Incubator)
dataset$Mean_temperature <- as.factor(dataset$Mean_temperature)
dataset$Treatment <- as.factor(dataset$Treatment)
dataset$Label <- as.factor(dataset$Label)
```

Poisson selected because this is count data, right-skewed (lots of values close to zero)
```{r echo=FALSE}
dataset$total_living_fronds <- as.numeric(dataset$total_living_fronds)
par(mfrow = c(2, 2))
invisible(sapply(unique(dataset$Label), function(trt) {
  invisible(sapply(c("LM", "LP"), function(sp) {
    hist(dataset$total_living_fronds[dataset$Species == sp & dataset$Label == trt],
         main = paste("Species:", sp, "\nTreatment:", trt),  
         xlab = "Live fronds", col = "lightblue", border = "white",
         breaks = seq(0, max(dataset$total_living_fronds, na.rm = TRUE) + 10, by = 10))
  }))
}))
```

## View number of replicates
LM
```{r echo=FALSE}
table(dataset$Mean_temperature[dataset$Species == "LM"], dataset$Treatment[dataset$Species == "LM"])
```

LP
```{r echo=FALSE}
table(dataset$Mean_temperature[dataset$Species == "LP"], dataset$Treatment[dataset$Species == "LP"])
```

### Model fitting and validation

```{r echo=TRUE}
model_results <- list()
compare_results <- list()

species_list <- list("LM", "LP")

for (sp in species_list) {
  species_data <- subset(dataset, Species == sp)
  
  # Assigning new IDs to experiment runs to ensure IDs are unique (as different
  # average temperatures had same IDs)
  species_data$Exp_run <- rep(seq(1, length(species_data$Exp_run) / 3), each = 3)

  library(lme4)

# Fit the models
simple <- glm(total_living_fronds ~ Label*Mean_temperature, data=species_data, family=poisson)
exp_number <- glmer(total_living_fronds ~ Label*Mean_temperature + (1|Exp_run), data=species_data, family=poisson, control = glmerControl(optimizer = "nloptwrap"))

# Compare simple and mixed model
anova(exp_number, simple, test = "Chisq")
L0 <- logLik(simple)
L1 <- logLik(exp_number)
L.ratio <- as.vector(- 2 * (L0 - L1))
p_value <- 0.5 * (1 - pchisq(L.ratio, 1))
options(scipen = 999)
print(paste("Comparing simple x mixed model:"))
cat("p_value:", format(p_value, digits = 10), "\n")

# Store models
model_results[[sp]] <- list(simple = simple, exp_number = exp_number)

# Compare models by AIC
Cand.modsF <- list("no random effects" = model_results[[sp]]$simple,
                   "experiment number" = model_results[[sp]]$exp_number)

# Get AIC for each model and store it
AIC_values <- sapply(Cand.modsF, function(model) AIC(model))

# Create a summary table
compare_results[[sp]] <- knitr::kable(data.frame(Model = names(AIC_values), AIC = AIC_values), "simple", digits = 2)

 cat("Model assessment:", sp, "\n")
 print(compare_results[[sp]])

# Find the best model based on AIC
best_model_name <- names(AIC_values)[which.min(AIC_values)]
model_results[[sp]] <- Cand.modsF[[best_model_name]]

  best_model <- model_results[[sp]]
  
  model_dispersion <- sum(residuals(best_model, type = "pearson")^2) / df.residual(best_model)
  print(paste("Model dispersion ratio for species", sp, ":", model_dispersion))
  
  cat("Wald test type 3 for significance of predictor:", sp, "\n")
  library(car)
phi <- sum(residuals(best_model, type="pearson")^2) / df.residual(best_model)  
anova_table <- Anova(best_model, type = "III")  

# Adjusting chi-square values (quasipoisson)
anova_table$`Chisq` <- anova_table$`Chisq` / phi  

# Recalculating p-values using chi-square distribution
anova_table$`Pr(>Chisq)` <- pchisq(anova_table$`Chisq`, anova_table$Df, lower.tail = FALSE)  

print(anova_table)
}
```

## Model validation

All assumptions were met.

```{r echo=FALSE}
validation_results <- list()

for (sp in species_list) {
  best_model <- model_results[[sp]]
  # Model validation for each species
  model_dispersion <- sum(residuals(best_model, type = "pearson")^2) / df.residual(best_model)
  print(paste("Model dispersion ratio for species", sp, ":", model_dispersion))
  
  # Deviance of the best model
best_model_deviance <- deviance(best_model)

species_data <- subset(dataset, Species == sp)
# Fit the null model (only the intercept)
null_model <- glm(total_living_fronds ~ 1, family = poisson, data = species_data)

# Deviance of the null model
null_model_deviance <- deviance(null_model)

# Calculate pseudo R^2 based on deviance
pseudo_R2_deviance <- 1 - (best_model_deviance / null_model_deviance)

# Print the results
cat(sp, "\n")
cat("Deviance of the best model:", best_model_deviance, "\n")
cat("Deviance of the null model:", null_model_deviance, "\n")
cat("Pseudo R^2 based on deviance:", pseudo_R2_deviance, "\n")

# Residual Diagnostics
par(mfrow = c(2, 2))
plot(fitted(best_model), residuals(best_model), 
     xlab = "Fitted Values", ylab = "Residuals", main = "Residuals vs Fitted")
abline(h = 0, col = "red")
hist(residuals(best_model, type = "deviance"), 
     main = "Deviance Residuals", xlab = "Residuals")
cooks_dist <- cooks.distance(best_model)
plot(cooks_dist, type = "h", main = "Cook's Distance", ylab = "Cook's Distance")
abline(h = 4 / nrow(species_data), col = "red", lty = 2)
}
```

# Adjusting model results to account for overdispersion
```{r echo=TRUE}
for (sp in species_list) {
  full_mod1 <- model_results[[sp]]
quasi_table <- function(model,ctab=coef(summary(model))) {
  phi <- sum(residuals(model, type="pearson")^2)/df.residual(model)
  qctab <- within(as.data.frame(ctab),
                  {   `Std. Error` <- `Std. Error`*sqrt(phi)
                  `z value` <- Estimate/`Std. Error`
                  `Pr(>|z|)` <- 2*pnorm(abs(`z value`), lower.tail=FALSE)
                  })
  return(qctab)
}

print(paste("Adjusted model summary - species :", sp, ":"))
printCoefmat(quasi_table(full_mod1),digits=2)
#As specified, the estimates are identical; 
#the standard errors and p-values have been appropriately 
#inflated, the z-values have been appropriately deflated.
#https://stackoverflow.com/questions/68915173/how-do-i-fit-a-quasi-poisson-model-with-lme4-or-glmmtmb
# Checks for Overdispersion: It calculates a number (phi) that tells you how much the data spreads out more than expected.
# Fixes Standard Errors: It takes the standard errors from the model and makes them bigger based on phi.
# Recalculates Other Numbers: Since standard errors changed, it also updates the z-values and p-values.
# Returns the New Table: It gives you a corrected version of the table with the adjusted values.
# Basically, your Poisson model didnâ€™t fit well because the data was too spread out, so this function helps fix that by making sure the errors and test results reflect the extra spread
}
```
